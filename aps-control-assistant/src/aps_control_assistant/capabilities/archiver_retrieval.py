"""
Archiver Data Retrieval Capability

This capability retrieves historical time-series data from the archiver.
It provides access to archived data for analysis, plotting, and trend monitoring.

Based on ALS Assistant's Get Archiver Data capability pattern.

Configuration:
    The archiver connector is configured in config.yml. By default, the template
    uses the mock archiver connector which works with any channel names.

    Production Mode (config.yml):
        archiver:
            type: epics_archiver
            epics_archiver:
                url: https://pvarchiver.aps.anl.gov   # optionally include /retrieval
                timeout: 60
                verify_ssl: false

    The capability code remains the same - just change the config!
"""

import asyncio
import textwrap
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional

from osprey.base.capability import BaseCapability
from osprey.base.decorators import capability_node
from osprey.base.errors import ErrorClassification, ErrorSeverity
from osprey.base.examples import (
    ClassifierActions,
    ClassifierExample,
    OrchestratorExample,
    OrchestratorGuide,
    TaskClassifierGuide,
)
from osprey.base.planning import PlannedStep
from osprey.utils.config import _get_config

from ..context_classes import ArchiverDataContext
from .aps_archiver import ArchiverRequestError, fetch_archiver_history



# === Archiver-Related Errors ===
class ArchiverError(Exception):
    """Base class for all archiver-related errors."""
    pass

class ArchiverTimeoutError(ArchiverError):
    """Raised when archiver requests time out."""
    pass

class ArchiverConnectionError(ArchiverError):
    """Raised when archiver connectivity issues."""
    pass

class ArchiverDataError(ArchiverError):
    """Raised when archiver returns unexpected data format."""
    pass

class ArchiverDependencyError(ArchiverError):
    """Raised when required dependencies are missing."""
    pass


# ========================================================
# Capability Implementation
# ========================================================

@capability_node
class ArchiverRetrievalCapability(BaseCapability):
    """
    Archiver Data Retrieval Capability with production patterns.

    - Complete archiver service integration with error handling
    - Comprehensive validation and timeout protection
    - Registry-based patterns for context types
    - Rich orchestrator examples and classifier configuration
    - Automatic context management with dependency validation
    """

    name = "archiver_retrieval"
    description = "Retrieve historical channel data from the archiver"
    provides = ["ARCHIVER_DATA"]
    requires = ["CHANNEL_ADDRESSES", ("TIME_RANGE", "single")]

    async def execute(self) -> Dict[str, Any]:
        """
        Retrieve historical channel data from the archiver service.

        This method queries the configured archiver (mock or EPICS) for historical
        time-series data across specified channels and time ranges. It automatically
        extracts required inputs, handles data conversion, and provides structured results.

        Returns:
            State updates containing ARCHIVER_DATA context with historical time series.

        Raises:
            ArchiverDependencyError: If required contexts are missing or invalid.
            ArchiverConnectionError: If archiver service is unreachable.
            ArchiverDataError: If data retrieval fails.
        """

        # Get unified logger with automatic streaming
        logger = self.get_logger()

        # Get task description for logging
        task_objective = self.get_task_objective(default='unknown')
        logger.info(f"Starting archiver data retrieval: {task_objective}")

        # Some logger implementations support `status()`, but standard logging.Logger does not.
        # Use a safe fallback to INFO when `status` is unavailable.
        status_message = "Initializing archiver data retrieval..."
        if hasattr(logger, "status"):
            logger.status(status_message)
        else:
            logger.info(status_message)

        try:
            # Extract required contexts from execution state
            try:
                channels_to_retrieve, time_range_context = self.get_required_contexts()
                logger.info(f"Successfully extracted both required contexts: CHANNEL_ADDRESSES and TIME_RANGE")
            except ValueError as e:
                raise ArchiverDependencyError(str(e))

            # Validate that we have channel addresses to work with
            if not channels_to_retrieve or len(channels_to_retrieve) == 0:
                raise ArchiverDependencyError("No channel addresses available for archiver data retrieval. The channel finding step may have failed to locate suitable channels.")

            status_message = f"Found {len(channels_to_retrieve)} channels, retrieving data..."
            if hasattr(logger, "status"):
                logger.status(status_message)
            else:
                logger.info(status_message)

            logger.debug(f"Retrieving archiver data for {len(channels_to_retrieve)} channels from {time_range_context.start_date} to {time_range_context.end_date}")

            # Extract optional parameters
            params = self.get_parameters()
            precision_ms = params.get('precision_ms', 1000)

            config_builder = _get_config()
            archiver_cfg = config_builder.get('archiver', {}) or {}
            epics_cfg = archiver_cfg.get('epics_archiver', {}) if isinstance(archiver_cfg, dict) else {}
            base_url = epics_cfg.get('url') or "http://pvarchiver.aps.anl.gov:17668/retrieval"
            timeout = epics_cfg.get('timeout', 60)
            verify_ssl = epics_cfg.get('verify_ssl', False)
            local_tz = archiver_cfg.get('local_timezone', "America/Chicago")

            # Retrieve the data from archiver (returns pandas DataFrame)
            try:
                archiver_df = await asyncio.to_thread(
                    fetch_archiver_history,
                    channels_to_retrieve,
                    time_range_context.start_date,
                    time_range_context.end_date,
                    base_url,
                    timeout,
                    verify_ssl,
                    local_tz,
                )
            except ArchiverRequestError as exc:
                raise ArchiverConnectionError(str(exc)) from exc

            if archiver_df.empty:
                raise ArchiverDataError("Archiver returned no data for the requested time range.")

            status_message = "Converting archiver data to structured format..."
            if hasattr(logger, "status"):
                logger.status(status_message)
            else:
                logger.info(status_message)

            # Extract timestamps from DataFrame index
            timestamps = [ts.to_pydatetime() for ts in archiver_df.index]

            # Extract time series data for each channel
            time_series_data = {
                channel: archiver_df[channel].tolist()
                for channel in channels_to_retrieve
                if channel in archiver_df.columns
            }

            # Optional: persist CSV and plot to disk for downstream use
            save_outputs = params.get("save_outputs", True)

            # If using a real EPICS archiver, default to not generating plots unless explicitly requested
            archiver_type = archiver_cfg.get("type") if isinstance(archiver_cfg, dict) else None
            if archiver_type == "epics_archiver":
                save_outputs = params.get("save_outputs", False)

            output_dir_param = params.get("output_dir")
            if save_outputs:
                project_root = Path(config_builder.get("project_root"))
                timestamp_label = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
                output_dir = Path(output_dir_param) if output_dir_param else project_root / "build" / "archiver_outputs" / timestamp_label
                output_dir.mkdir(parents=True, exist_ok=True)

                csv_path = output_dir / "archiver_data.csv"
                try:
                    archiver_df.to_csv(csv_path)
                    logger.info(f"Saved archiver data CSV to {csv_path}")
                except Exception as exc:
                    logger.warning(f"Failed to write archiver CSV: {exc}")

                # Attempt to plot using matplotlib if available, but avoid huge plots
                try:
                    import matplotlib.pyplot as plt  # type: ignore

                    num_points = len(archiver_df.index)
                    max_plot_points = 200000
                    if num_points > max_plot_points:
                        logger.warning(
                            f"Skipping plot generation: {num_points} points exceed plot limit "
                            f"({max_plot_points}); returning data without a PNG plot."
                        )
                    else:
                        plt.figure(figsize=(10, 5))
                        for channel in channels_to_retrieve:
                            if channel in archiver_df.columns:
                                plt.plot(archiver_df.index, archiver_df[channel], label=channel)

                        plt.xlabel("Time (UTC)")
                        plt.ylabel("Value")
                        plt.title("Archiver Data")
                        plt.legend()
                        plt.grid(True)
                        plt.tight_layout()

                        plot_path = output_dir / "archiver_plot.png"
                        plt.savefig(plot_path, dpi=150)
                        plt.close()
                        logger.info(f"Saved archiver plot to {plot_path}")
                except Exception as exc:
                    logger.warning(f"Failed to generate archiver plot: {exc}")

            logger.debug(f"Retrieved archiver data with {len(timestamps)} timestamps and {len(time_series_data)} channels")

            status_message = "Creating archiver data context..."
            if hasattr(logger, "status"):
                logger.status(status_message)
            else:
                logger.info(status_message)

            # Create rich context object
            archiver_context = ArchiverDataContext(
                timestamps=timestamps,
                precision_ms=precision_ms,
                time_series_data=time_series_data,
                available_channels=list(time_series_data.keys())
            )

            # Log archiver data info with safe timestamp access
            start_time = archiver_context.timestamps[0] if archiver_context.timestamps else 'N/A'
            end_time = archiver_context.timestamps[-1] if archiver_context.timestamps else 'N/A'
            logger.info(f"Retrieved archiver data: {len(archiver_context.timestamps)} points for {len(archiver_context.available_channels)} channels from {start_time} to {end_time}")

            # Store result in execution context
            return self.store_output_context(archiver_context)

        except Exception as e:
            logger.error(f"Archiver data retrieval failed: {e}")
            logger.error(f"Archiver data retrieval failed: {str(e)}")
            raise

    def process_extracted_contexts(self, contexts):
        """
        Flatten CHANNEL_ADDRESSES contexts into single list.

        **HOOK METHOD**: This method is automatically called by get_required_contexts()
        after extracting contexts from state but before returning them to execute().
        Override this method to customize how contexts are processed.

        In this capability, we need to handle the case where multiple CHANNEL_ADDRESSES
        contexts exist (e.g., from multiple channel_finding steps). This hook flattens
        them into a single list of channel strings for archiver retrieval.

        Args:
            contexts: Dict mapping context type names to extracted context objects
                     (may contain lists if multiple contexts of same type exist)

        Returns:
            Processed contexts dict (CHANNEL_ADDRESSES converted to flat list of strings)

        Note:
            Without this override, channels_to_retrieve would be a ChannelAddressesContext
            object (or list of them), but archiver expects a flat list of channel strings.
        """
        channels_raw = contexts["CHANNEL_ADDRESSES"]

        if isinstance(channels_raw, list):
            # Flatten multiple contexts into single channel list
            channels_flat = []
            for ctx in channels_raw:
                channels_flat.extend(ctx.channels)
            self.get_logger().info(f"Merged {len(channels_raw)} CHANNEL_ADDRESSES contexts into {len(channels_flat)} total channels")
            contexts["CHANNEL_ADDRESSES"] = channels_flat
        else:
            # Single context - extract channel list
            contexts["CHANNEL_ADDRESSES"] = channels_raw.channels

        return contexts

    @staticmethod
    def classify_error(exc: Exception, context: dict) -> ErrorClassification:
        """
        Domain-specific error classification with detailed recovery suggestions.
        """

        if isinstance(exc, ArchiverTimeoutError):
            return ErrorClassification(
                severity=ErrorSeverity.RETRIABLE,
                user_message=f"Archiver timeout error: {str(exc)}",
                metadata={
                    "technical_details": "Archiver requests timed out",
                    "suggestions": [
                        "Reduce the time range of your query",
                        "Request fewer channels in a single query",
                        "Increase precision_ms parameter to reduce data points",
                    ]
                }
            )
        elif isinstance(exc, ArchiverConnectionError):
            return ErrorClassification(
                severity=ErrorSeverity.CRITICAL,
                user_message=f"Archiver connection error: {str(exc)}",
                metadata={
                    "technical_details": "Cannot establish connection to archiver service",
                    "suggestions": [
                        "Verify the archiver service is accessible",
                        "Check network connectivity",
                        "Contact operations if archiver service appears down",
                    ]
                }
            )
        elif isinstance(exc, ArchiverDataError):
            return ErrorClassification(
                severity=ErrorSeverity.REPLANNING,
                user_message=f"Archiver data error: {str(exc)}",
                metadata={
                    "technical_details": "Archiver returned unexpected data format",
                    "replanning_reason": f"Archiver data format issue: {exc}",
                    "suggestions": [
                        "Verify that the requested channel names exist and are archived",
                        "Check if the time range contains actual data",
                        "Try a different time range",
                    ]
                }
            )
        elif isinstance(exc, ArchiverDependencyError):
            return ErrorClassification(
                severity=ErrorSeverity.REPLANNING,
                user_message=f"Missing dependency: {str(exc)}",
                metadata={
                    "technical_details": "Required input context (CHANNEL_ADDRESSES or TIME_RANGE) not available for archiver data retrieval",
                    "replanning_reason": f"Missing required inputs: {exc}",
                    "suggestions": [
                        "Ensure channel addresses have been found using channel_finding capability",
                        "Verify time range has been parsed using the time_range_parsing capability",
                        "Check that required input contexts are available from previous steps",
                    ]
                }
            )
        elif isinstance(exc, ArchiverError):
            # Generic archiver error
            return ErrorClassification(
                severity=ErrorSeverity.RETRIABLE,
                user_message=f"Archiver error: {str(exc)}",
                metadata={
                    "technical_details": "General archiver service error",
                    "suggestions": [
                        "Retry the request as this may be a temporary service issue",
                        "Simplify the query by reducing time range or number of channels",
                    ]
                }
            )
        else:
            return ErrorClassification(
                severity=ErrorSeverity.CRITICAL,
                user_message=f"Archiver data retrieval failed: {exc}",
                metadata={"technical_details": str(exc)}
            )

    def _create_orchestrator_guide(self) -> Optional[OrchestratorGuide]:
        """Create prompt snippet for archiver data capability."""

        # Define structured examples
        archiver_retrieval_example = OrchestratorExample(
            step=PlannedStep(
                context_key="historical_beam_current_data",
                capability="archiver_retrieval",
                task_objective="Retrieve historical beam current data from archiver for the last 24 hours",
                expected_output="ARCHIVER_DATA",
                success_criteria="Historical data retrieved successfully for specified time range",
                inputs=[
                    {"CHANNEL_ADDRESSES": "beam_current_channels"},
                    {"TIME_RANGE": "last_24_hours_timerange"}
                ]
            ),
            scenario_description="Retrieve historical time-series data from the archiver",
            notes="Requires channel addresses and time range from previous steps. Output stored under ARCHIVER_DATA context type. Optional parameter: precision_ms (default: 1000)."
        )

        # Workflow example: Show what comes AFTER archiver_retrieval for plotting
        plotting_workflow_python_step = OrchestratorExample(
            step=PlannedStep(
                context_key="beam_current_plot",
                capability="python",
                task_objective="Create a matplotlib time-series plot of the beam current data showing trends over the 24-hour period",
                expected_output="PYTHON_RESULTS",
                success_criteria="Time-series plot created with proper labels, showing beam current trends",
                inputs=[{"ARCHIVER_DATA": "historical_beam_current_data"}]
            ),
            scenario_description="WORKFLOW: Use python capability to plot archiver data from previous step",
            notes="Typical plotting workflow: archiver_retrieval (gets data) → python (creates plot) → respond (delivers to user). The python capability consumes the ARCHIVER_DATA from the archiver_retrieval step."
        )

        # Workflow example: Show what comes AFTER archiver_retrieval for analysis
        analysis_workflow_python_step = OrchestratorExample(
            step=PlannedStep(
                context_key="beam_current_statistics",
                capability="python",
                task_objective="Calculate mean, standard deviation, min, and max values of the beam current data over the time period",
                expected_output="PYTHON_RESULTS",
                success_criteria="Statistical metrics calculated and displayed with clear labels",
                inputs=[{"ARCHIVER_DATA": "historical_beam_current_data"}]
            ),
            scenario_description="WORKFLOW: Use python capability to analyze archiver data from previous step",
            notes="Typical analysis workflow: archiver_retrieval (gets data) → python (calculates statistics) → respond (delivers results). The python capability consumes the ARCHIVER_DATA from the archiver_retrieval step."
        )

        return OrchestratorGuide(
            instructions=textwrap.dedent("""
                **When to plan "archiver_retrieval" steps:**
                - When tasks require historical channel data
                - When retrieving past values from the archiver
                - When time-series data is needed from archived sources

                **Step Structure:**
                - context_key: Unique identifier for output (e.g., "historical_data", "trend_data")
                - inputs: Specify required inputs:
                {"CHANNEL_ADDRESSES": "context_key_with_channel_data", "TIME_RANGE": "context_key_with_time_range"}

                **Required Inputs:**
                - CHANNEL_ADDRESSES data: typically from a "channel_finding" step
                - TIME_RANGE data: typically from a "time_range_parsing" step

                **Input flow and sequencing:**
                1. "channel_finding" step must precede this step (if CHANNEL_ADDRESSES data is not present already)
                2. "time_range_parsing" step must precede this step (if TIME_RANGE data is not present already)

                **Output: ARCHIVER_DATA**
                - Contains: Structured historical data from the archiver
                - Available to downstream steps via context system

                **Common downstream workflow patterns:**
                - For plotting requests: archiver_retrieval → python (create plot) → respond
                - For analysis/statistics: archiver_retrieval → python (calculate stats) → respond
                - For complex analysis: archiver_retrieval → data_analysis → respond
                - Combined: archiver_retrieval → data_analysis → python (plot analysis) → respond

                Do NOT plan this for current values; use "channel_value_retrieval" for real-time data.
                """),
            examples=[archiver_retrieval_example, plotting_workflow_python_step, analysis_workflow_python_step],
            priority=15
        )

    def _create_classifier_guide(self) -> Optional[TaskClassifierGuide]:
        """Create classifier for archiver data capability."""
        return TaskClassifierGuide(
            instructions="Determines if the task requires accessing the archiver. This is relevant for requests involving historical data or trends.",
            examples=[
                ClassifierExample(
                    query="Which tools do you have?",
                    result=False,
                    reason="This is a question about the AI's capabilities."
                ),
                ClassifierExample(
                    query="Plot the historical data for vacuum pressure for the last week.",
                    result=True,
                    reason="The query explicitly asks for historical data plotting."
                ),
                ClassifierExample(
                    query="What is the current beam energy?",
                    result=False,
                    reason="The query asks for a current value, not historical data."
                ),
                ClassifierExample(
                    query="Can you plot that over the last 4h?",
                    result=True,
                    reason="The query asks for historical data plotting."
                ),
                ClassifierExample(
                    query="What was that value yesterday?",
                    result=True,
                    reason="The query asks for historical data."
                ),
            ],
            actions_if_true=ClassifierActions()
        )
